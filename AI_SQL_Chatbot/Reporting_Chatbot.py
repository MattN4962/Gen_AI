from bs4 import BeautifulSoup
import markdown
import mysql.connector
import pandas as pd
from transformers import pipeline
from dotenv import load_dotenv
import gradio as gr
import streamlit as st
import re
import os
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table
from reportlab.lib.styles import getSampleStyleSheet
from transformers import AutoTokenizer, AutoModelForCausalLM
from openai import AzureOpenAI
import pyodbc
#This file is just to test the connection and a simple query
# Database connection configuration

load_dotenv()
api_version = os.getenv("API_VERSION")
api_key = os.getenv("API_KEY")
azure_url = os.getenv("AZURE_URL")
sql_password = os.getenv("MY_SQL_PASSWORD")
azure_server = os.getenv("AZURE_SYNAPSE_SERVER")
azure_db = os.getenv("AZURE_SYNAPSE_DB")
email = os.getenv("EMAIL")

config = {
    'user': 'root',
    'password': sql_password,
    'host': 'localhost',
    'port': 3306,
    'database':'Main'
}

conn_string = (
    "Driver={ODBC Driver 18 for SQL Server};"
    f"Server=tcp:{azure_server},1433;"
    f"Database={azure_db};"
    "Encrypt=yes;"
    "TrustServerCertificate=no;"
    "Connection Timeout=30;"
    "Authentication=ActiveDirectoryInteractive;"
    f"UID={email};"
)
client = AzureOpenAI(
    api_version=api_version,
    azure_endpoint=azure_url,
    api_key=api_key
)
#Comment
def get_db_connection():
    try:
        conn = pyodbc.connect(conn_string)
        cursor = conn.cursor()
        print("Database connection established.")
        return conn, cursor
    except pyodbc.Error as err:
        print(f"Error: {err}")
        return None, None

def get_schema():
    query = """
        SELECT
            TABLE_SCHEMA AS table_schema,
            TABLE_NAME   AS table_name,
            COLUMN_NAME  AS column_name,
            DATA_TYPE    AS data_type
        FROM INFORMATION_SCHEMA.COLUMNS
        WHERE TABLE_SCHEMA IN ('CustomerHub', 'OrderHub')
        ORDER BY TABLE_SCHEMA, TABLE_NAME, ORDINAL_POSITION;
    """

    conn, cursor = get_db_connection()
    cursor.execute(query)
    rows = [tuple(row) for row in cursor.fetchall()]

    df = pd.DataFrame(rows, columns=["table_schema","table_name", "column_name", "data_type"])
    df["full_table_name"] = df["table_schema"] + "." + df["table_name"]
    #print(df)
    schema_dict = (
            df.groupby("full_table_name")
              .apply(lambda g: [f"{col} ({dtype})" for col, dtype in zip(g.column_name, g.data_type)])
              .to_dict()
        )
    return schema_dict

def is_Sql_safe(query: str) -> bool:
    # Define a list of forbidden keywords
    forbidden_keywords = ['DROP', 'DELETE', 'INSERT', 'UPDATE', 'ALTER', 'CREATE', 'TRUNCATE', 'EXEC', 'EXECUTE']
    
    clean_sql = re.sub(r"```sql|```", "", query, flags=re.IGNORECASE).strip()

    # Check for forbidden keywords in the query (case-insensitive)
    if any(word in clean_sql for word in forbidden_keywords):
        return False
    if not clean_sql.startswith("SELECT"):
        return False
    return True

def fix_sql_identifiers(sql: str) -> str:
    """
    Ensures that all schema-qualified table names like CustomerHub.Table
    are wrapped in [CustomerHub].[Table] to avoid reserved keyword issues.
    """
    # Pattern matches: Schema.Table (with letters, numbers, underscores)
    pattern = r'\b([A-Za-z0-9_]+)\.([A-Za-z0-9_]+)\b'

    def replace(match):
        schema, table = match.groups()
        return f'[{schema}].[{table}]'
#comment
    fixed_sql = re.sub(pattern, replace, sql)
    return fixed_sql

# Returns SQL query generated by the LLM
def generate_sql(user_query: str, schema: dict) -> str:
    response = client.chat.completions.create(
        messages=[
            {
                "role":"system",
                "content":f"""You will take the user query in natural language and produce a a SINGLE SQL query using only SELECT statements using SQL Server query syntax."
                " This is the table schema: {schema}, and the field is_Pro is either True or False."
                "When placing brackets around schema and table names, make sure that ONLY table and schema names are in []'s" 
                "Always ensure joins and filters compare columns of the same data type. "
                "If joining numeric and string columns, use TRY_CAST on both sides to avoid conversion errors."
                "If a column has a datetimeoffset data type, always CAST it as DATETIME in the query "
                "to ensure compatibility with pyodbc and Python."
                "Only use columns that you know exist in the provided schema. Do NOT assume and columns exist if they are not listed"  
                """
            },
            {
                "role":"user",
                "content":user_query
            }
        ],
        temperature=0,
        max_tokens=200,
        model="gpt-4o"
    )
    query = re.sub(r"```sql|```", "", response.choices[0].message.content, flags=re.IGNORECASE).strip()
    query = fix_sql_identifiers(query) 
    return query

# Executes the SQL query and returns the results as a DataFrame
def run_SQL_query(sql: str) -> pd.DataFrame:
    conn, cursor = get_db_connection()
    if conn is None or cursor is None:
        return pd.DataFrame()
    
    if not is_Sql_safe(sql):
        print("Unsafe SQL query detected.")
        connection_cleanup(conn, cursor)
        return pd.DataFrame()
    
    try:
        cursor.execute(sql)
        rows = cursor.fetchall()
        # Make sure each row is a tuple/list
        data = [tuple(row) if not isinstance(row, tuple) else row for row in rows]
        columns = [column[0] for column in cursor.description]
        df = pd.DataFrame(data, columns=columns)
        return df
    except pyodbc.Error as err:
        print(f"SQL Error: {err}")

        # if "nvarchar" in err.lower() and "bigint" in err.lower():
        #     print("Retrying query with TRY_CAST for possible type mismatch...")
        #     safe_sql = re.sub(r"=\s*([\w\.]+)", r"= TRY_CAST(\1 AS BIGINT)", sql)
        #     try:
        #         cursor.execute(safe_sql)
        #         results = cursor.fetchall()
        #         return pd.DataFrame(results)
        #     except Exception as e2:
        #         print(f"Retry failed: {e2}")
        #         return pd.DataFrame()

        return pd.DataFrame()
    
    finally:
        connection_cleanup(conn, cursor)

# Close all connections
def connection_cleanup(conn, cursor):
    cursor.close()
    conn.close()
    print("Database connection closed.")

def generate_insights(df: pd.DataFrame, user_question: str):
    if df.empty:
        return "No query results so no insights report can be given"
    data_sample = df.head(30).to_markdown(index=False)

    prompt = (
        f"You are a marketing analyst. The user asked: '{user_question}'.\n"
        f"Here are the first rows of the query results:\n{data_sample}\n\n"
        "Provide a concise marketing analysis and suggestions based on these results. "
        "Focus on actionable strategies such as promotions, customer segmentation, "
        "or product opportunities. Write in clear natural language."
    )
    response = client.chat.completions.create(
        messages=[
            {"role": "system", "content": "You are an expert marketing analyst."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.6,
        max_tokens=1000,
        model="gpt-4o"
    )

    return response.choices[0].message.content.strip()

def generate_report(df: pd.DataFrame, filename: str):
    doc = SimpleDocTemplate(filename)
    styles = getSampleStyleSheet()
    flow = [Paragraph("SQL Query Results Report", styles['Title']), Spacer(1, 12)]

    if not df.empty:
        data = [df.columns.tolist()] + df.values.tolist()
        flow.append(Paragraph("Table Data", styles["Heading2"]))
        flow.append(Table(data)) 
    else:
        flow.append(Paragraph("No Table Data Returned", styles["Normal"]))
    
    doc.build(flow)

get_db_connection()


# schema = get_schema()
# query = generate_sql("Show me the first 10 Silver loyalty members", schema=schema)
# print(query)

# df = run_SQL_query(query)
# print(df)

# st.title("On-Demand SQL Reporting Bot")
# st.markdown("Type a natural language question to query your database")


# st.session_state.setdefault("last_df", None)
# st.session_state.setdefault("insighs", None)
# st.session_state.setdefault("reporting_bytes", None)

# user_query = st.text_area("Enter your question")

# if st.button("Run Query"):
#     schema = get_schema()
#     if not schema:
#         st.error("Unable to get schema")
#     else:
#         sql = generate_sql(user_query=user_query, schema=schema)
#         st.code(sql, language="sql")
        
#         df = run_SQL_query(sql=sql)
#         if not df.empty:
#             st.session_state["last_df"] = df
#             st.session_state["insights"] = None
#             st.session_state["report_bytes"] = None
#         else:
#             st.warning("No results found or unsafe query")

# if st.session_state['last_df'] is not None and not st.session_state['last_df'].empty:
#     st.subheader("Query Results")
#     st.dataframe(st.session_state["last_df"])

#     #Table download
#     csv_data = st.session_state["last_df"].to_csv(index=False, encoding="utf-8")
#     st.download_button(
#         "Download Table Data (CSV)",
#         csv_data,
#         file_name="query_results.csv",
#         mime="text/csv"
#     )
    
#     if st.button("Generate Marketing Report"):
#         # 1. Get natural language analysis
#         insights = generate_insights(st.session_state['last_df'], user_query)
#         st.session_state["insights"] = insights

#         # 2. Create PDF with insights + table
#         pdf_path = "marketing_report.pdf"
#         generate_report(st.session_state['last_df'], pdf_path)
#         with open(pdf_path, "rb") as f:
#             st.session_state["report_bytes"] = f.read()

#         # 3. Allow download
#         if st.session_state.get("insights"):
#             st.subheader("Marketing Insights")
#             st.markdown(st.session_state["insights"])

        


